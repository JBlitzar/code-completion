K=10,T=0.8: <data>jnp . square ( jax . nn . relu ( x 1 - x 2 ) ) / dispersion <newline> return jnp . exp ( d 2 - jnp . <UNK> ( d , d 2 ) ) <newline> <newline> <newline> def _ swap _ prob _ entropy _ reg _ <UNK> ( x 1 , x 2 , dispersion = 1 . 0 , norm _ p = 1 . 0 ) : <newline> <newline> d = 2 * jnp . <UNK> ( jax . nn . relu ( x 2 - x 1 ) , norm _ p </data><tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><UNK> <UNK> small will <UNK> a large learning rate and will <newline> <tab> <tab> <UNK> <UNK> results . <newline> <tab> <tab> <UNK> _ <UNK> : if true , <UNK> <UNK> <UNK> if gradient <UNK> <UNK> <UNK> . <newline> <tab> <tab> initial _ const : the initial <UNK> - constant to use to <UNK> the <UNK> <newline> <tab> <tab> <UNK> of distance and confidence . should be set to a <UNK> small <newline> <tab> <tab> value ( but <UNK> ) . <newline> <tab> <tab> <UNK> _ const : the <UNK> constant to use <UNK> we <UNK> <UNK> . should <newline> <tab> </data><tab> <tab> <tab> <newline> ( <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <UNK> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> _ <tab> <tab> <tab> <tab> <tab> . _ <newline> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <UNK> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <UNK> _ _ <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>jnp . square ( jax . nn . relu ( x 1 - x 2 ) ) / dispersion <newline> return jnp . exp ( d 2 - jnp . <UNK> ( d , d 2 ) ) <newline> <newline> <newline> def _ swap _ prob _ entropy _ reg _ <UNK> ( x 1 , x 2 , dispersion = 1 . 0 , norm _ p = 1 . 0 ) : <newline> <newline> d = 2 * jnp . <UNK> ( jax . nn . relu ( x 2 - x 1 ) , norm _ p </data>= . ' . _ . . <UNK> _ <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> . ) . _ <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>real _ length ) ) <newline> <tab> <tab> <tab> else : <newline> <tab> <tab> <tab> <tab> starting _ point = [ 0 ] <newline> <tab> <tab> else : <newline> <tab> <tab> <tab> if self . params [ " try _ <UNK> _ starting " ] : <newline> <tab> <tab> <tab> <tab> starting _ point = random . sample ( range ( real _ length ) , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> min ( self . params [ " num _ <UNK> _ starting " ] , real _ length ) ) <newline> <tab> <tab> <tab> </data><tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><newline> <tab> <tab> else : <newline> <tab> <tab> <tab> raise cnn not found error ( " cnn name not found ! " ) <newline> <newline> <tab> <tab> rnn = self . config [ ' train ' ] [ ' rnn ' ] [ ' name ' ] <newline> <tab> <tab> self . hidden _ num = int ( self . config [ ' train ' ] [ ' lstm ' ] [ ' hidden _ num ' ] ) <newline> <tab> <tab> dropout = int ( self . config [ ' train ' ] [ ' lstm ' ] [ </data>' ] , 1 , ' , <UNK> ( 2 2 2 , 0 ' , ' [ ' , ' , [ ' ] ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> output , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>" , caption ) <newline> <tab> caption = re . sub ( r " [ \ u 3 1 f 0 - \ u 3 1 ff ] + " , " " , caption ) <newline> <tab> caption = re . sub ( r " [ \ u 3 2 0 0 - \ u 3 2 ff ] + " , " " , caption ) <newline> <tab> caption = re . sub ( r " [ \ u 3 3 0 0 - \ u 3 3 ff ] + " , " " , caption ) </data><newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> if <newline> <tab> <tab> <newline> <tab> <tab> <newline> <tab> <tab> <tab> def <UNK> _ <UNK> " ) <newline> <tab> <tab> for i ( " , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> if <UNK> _ _ " , 1 2 " , 1 0 . 2 ) <newline> <tab> <tab> <tab> <tab> if <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><tab> <tab> " b 1 _ grad " : grad _ b 1 _ last , <newline> <tab> <tab> <tab> <tab> " w 2 _ grad " : grad _ w 2 _ last , <newline> <tab> <tab> <tab> <tab> " b 2 _ grad " : grad _ b 2 _ last , <newline> <tab> <tab> <tab> } <newline> <tab> <tab> <tab> return last _ param _ dict , xqw _ mini _ batch <newline> <newline> <tab> <tab> if last _ mini _ batch _ params _ dict is not none : <newline> <tab> <tab> <tab> init _ params </data>_ <UNK> ( 1 2 " ) : <newline> <tab> <tab> <tab> <tab> <tab> self . path = true : <newline> <tab> <tab> <tab> <tab> <tab> def _ dir = <UNK> , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><tab> if self . params [ ' use _ edge _ bias ' ] : <newline> <tab> <tab> <tab> <tab> <tab> <tab> m + = self . weights [ ' edge _ biases ' + scope _ name + str ( iter _ idx ) ] [ edge _ type ] <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> m = tf . reshape ( m , [ - 1 , v , h _ dim ] ) <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> </data><tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> if self ) <newline> <tab> <tab> <tab> self . <UNK> ' ] <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> for self . <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><tab> <tab> max _ iteration _ num = max ( d [ ' number _ iteration ' ] , max _ iteration _ num ) <newline> <tab> <tab> batch _ data = { ' adj _ mat ' : [ ] , ' init ' : [ ] , ' labels ' : [ ] , ' edge _ type _ masks ' : [ ] , ' edge _ type _ labels ' : [ ] , ' edge _ masks ' : [ ] , <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> ' edge _ labels </data>' ] ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> for i [ ' : <newline> <newline> <tab> <tab> res _ train _ params [ ' ] <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> self . append ( self . conv _ loss _ type _ <UNK> ' ] [ ' ] ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> self . shape [ ' <UNK> ' ] ) <newline> <tab> 
