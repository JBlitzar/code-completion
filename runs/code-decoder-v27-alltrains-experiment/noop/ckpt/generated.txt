K=10,T=0.8: <data>: <newline> <tab> <tab> wandb . log ( { " example " : wandb . video ( to _ save _ path ) } , step = step ) <newline> <newline> <newline> <newline> <newline> <newline> <newline> def main ( args ) : <newline> <tab> global logger <newline> <newline> <tab> assert torch . cuda . is _ available ( ) , " training <UNK> requires at <UNK> one gpu . " <newline> <newline> <tab> <newline> <tab> setup _ distributed ( ) <newline> <tab> <newline> <tab> <newline> <tab> <newline> <tab> <newline> <tab> <newline> <newline> <tab> rank = int ( os . environ </data>( _ <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <tab> <tab> <tab> <newline> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> _ <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>y * ttt _ output <newline> <tab> <tab> return output <newline> <newline> <tab> def get _ ttt _ inputs ( self , inputs , mini _ batch _ size , cache _ params ) : <newline> <tab> <tab> xq = inputs [ " xq " ] <newline> <tab> <tab> xk = inputs [ " xk " ] <newline> <tab> <tab> xv = inputs [ " xv " ] <newline> <tab> <tab> x = inputs [ " x " ] <newline> <tab> <tab> b , l , c = x . shape <newline> <tab> <tab> num _ mini _ batch = </data>" <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data><tab> <tab> <tab> lib . plot . plot ( ' wgan ' , _ disc _ wgan ) <newline> <tab> <tab> <tab> lib . plot . plot ( ' acgan ' , _ disc _ acgan ) <newline> <tab> <tab> <tab> lib . plot . plot ( ' acc _ real ' , _ disc _ acgan _ acc ) <newline> <tab> <tab> <tab> lib . plot . plot ( ' acc _ fake ' , _ disc _ acgan _ fake _ acc ) <newline> <tab> <tab> lib . plot . plot ( ' time ' , time . </data>( ' : <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> self . <UNK> ( 1 , ' ) <newline> <tab> <tab> <UNK> _ size _ ( ' ] ) ) ) <newline> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> self . ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> self . 0 , self . <UNK> 
K=10,T=0.8: <data>9 2 _ extra - b 0 4 0 e 4 a 9 b . pth ' , <newline> <tab> ' dpn 9 8 ' : <newline> <tab> <tab> ' https : / / github . com / <UNK> / pytorch - dpn - pretrained / <UNK> / download / v 0 . 1 / dpn 9 8 - 5 b 9 0 <UNK> 4 d . pth ' , <newline> <tab> ' dpn 1 3 1 ' : <newline> <tab> <tab> ' https : / / github . com / <UNK> / pytorch - dpn - pretrained / <UNK> </data>_ <UNK> ' 0 0 ] , <UNK> . get _ _ path , <newline> <tab> <newline> <tab> <tab> output = ' <UNK> ' ] ) <newline> <tab> output = 0 ] , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <UNK> ( ' , ' - 1 1 0 1 . <UNK> ) , 3 ' , 1 0 , ' ) <newline> <tab> <tab> <tab> <tab> <tab> self . <UNK> ' ) : <newline> <tab> <tab> <tab> <tab> if ' , [ . <UNK> . - 1 1 2 , <newline> <tab> <tab> <tab> <tab> for <UNK> ( 
K=10,T=0.8: <data>config . epochs : <newline> <tab> logging . info ( ' epoch % d : ' , checkpoint . state . epoch ) <newline> <tab> for b , ( inputs , labels ) in enumerate ( <newline> <tab> <tab> cpdata . load _ batches ( self . data [ ' train ' ] ) ) : <newline> <tab> <tab> loss , trainable _ params , new _ model _ state , optimizer _ state , mixed = update _ fn ( <newline> <tab> <tab> <tab> trainable _ params , fixed _ params , inputs , labels , model _ state </data>_ _ <UNK> ' <UNK> ' ] ) <newline> <tab> <tab> <UNK> ' : <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> if args . append ( ' , ' ] ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>minimize ( gen _ cost , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> var _ list = lib . params _ with _ name ( ' generator ' ) ) <newline> <tab> disc _ train _ op = tf . train . adam optimizer ( learning _ rate = 2 e - 4 , beta 1 = 0 . 5 ) . minimize ( disc _ cost , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> </data><tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> else : <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> print ( ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>= 2 ) <newline> <tab> <tab> decoded _ samples = [ ] <newline> <tab> <tab> for i in xrange ( len ( samples ) ) : <newline> <tab> <tab> <tab> decoded = [ ] <newline> <tab> <tab> <tab> for j in xrange ( len ( samples [ i ] ) ) : <newline> <tab> <tab> <tab> <tab> decoded . append ( inv _ charmap [ samples [ i ] [ j ] ] ) <newline> <tab> <tab> <tab> decoded _ samples . append ( tuple ( decoded ) ) <newline> <tab> <tab> return decoded _ samples <newline> <newline> <tab> gen </data>_ name = 1 ] ) <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> if list ) : <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
K=10,T=0.8: <data>size = 6 4 <newline> <newline> model = net ( ) <newline> model . to ( device ) <newline> criterion = nn . cross entropy loss ( ) <newline> optimizer = optim . sgd ( model . parameters ( ) , lr = 0 . 1 ) <newline> <newline> <newline> if args . local _ rank = = 0 : <newline> <tab> tb _ writer = <UNK> writer ( <UNK> = ' ddp - 3 ' ) <newline> <newline> train _ sampler = distributed sampler ( train _ dataset ) <newline> train _ loader = torch . utils . data </data>( ) <newline> <newline> <newline> <tab> <newline> <newline> <newline> def _ loss _ <UNK> ( args . cuda _ loss = true ) <newline> def <UNK> ( ) : <newline> <newline> from <UNK> _ path , config _ path , config . <UNK> _ _ <UNK> ( ) <newline> <newline> <newline> <newline> <newline> <newline> config : <newline> def train ( ' ) <newline> <newline> <tab> <tab> <newline> parser . cuda ( ) <newline> <tab> config . add _ _ loss _ steps _ _ steps : as ( ' ) as np . add _ path . add _ size 
K=10,T=0.8: <data><newline> <tab> <tab> <tab> <tab> <tab> " name " : " <UNK> net v 2 " , <newline> <tab> <tab> <tab> <tab> } , <newline> <tab> <tab> <tab> <tab> ' rnn ' : { <newline> <tab> <tab> <tab> <tab> <tab> " name " : " lstm " <newline> <tab> <tab> <tab> <tab> } , <newline> <tab> <tab> <tab> <tab> ' optimizer ' : ' momentum ' , <newline> <tab> <tab> <tab> <tab> " test _ step " : 1 0 0 0 , <newline> <tab> <tab> <tab> <tab> " target " : { <newline> <tab> <tab> <tab> <tab> <tab> " </data>: { } <newline> <tab> <tab> <tab> <tab> } " : " , <newline> <tab> <tab> <tab> <tab> } " : 0 0 0 , <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> " <newline> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> <tab> 
